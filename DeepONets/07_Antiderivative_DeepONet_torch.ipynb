{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operator Learning with DeepXDE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as  dt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental.ode import odeint\n",
    "from jax.config import config\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N$: Number of functions $u(x)$ in training dataset  \n",
    "$P$: Number fo points inside domain at which $G(u)$ is evaluated (output evaluations)  \n",
    "$m$: Number of points at which input function is evaluated  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "Effective data generation requires parallel data generation when training.  \n",
    "__ID__:  Python string that identifies sample of sample\n",
    "__train__: Trining data\n",
    "__validation__: Validation data points  \n",
    "\n",
    "We will access training and validation samples using `ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RBF kernel\n",
    "def RBF(x1, x2, params):\n",
    "    output_scale, lengthscales = params\n",
    "    diffs =jnp.expand_dims(x1 / lengthscales, 1) - \\\n",
    "           jnp.expand_dims(x2 / lengthscales, 0)\n",
    "    r2 =jnp.sum(diffs**2, axis=2)\n",
    "    return output_scale *jnp.exp(-0.5 * r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the corresponding 10000 ODE solutions by solving: \n",
    "\n",
    "$$\\frac{dv(x)}{dx}=u(x)$$\n",
    "\n",
    "Using an explicit Runge-Kutta method(RK45)â†’ JAX's odeint functiom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uv(x,u,y,v):\n",
    "    fig,ax = plt.subplots(figsize = (8,6))\n",
    "    with mpl.rc_context({'font.size':18}):\n",
    "        ax.plot(x,u,'k--',label = '$u(x) = ds/dx$',\n",
    "                linewidth = 1.5)\n",
    "        ax.plot(y,v,'o--',label = '$s(x)=s(0) + \\int u(t)dt|_{t=y}$',\n",
    "                linewidth = 1.5)\n",
    "        ax.set_label('x')\n",
    "        ax.set_ylabel('u')\n",
    "        ax.tick_params(axis = 'y',color = 'black')\n",
    "        ax.legend(loc = 'lower right',ncol=1)\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geneate training data corresponding to one input sample\n",
    "def generate_one_training_data(key, m=100, P=1,\n",
    "                               length_scale=0.2):\n",
    "    # Sample GP prior at a fine grid\n",
    "    N = 512\n",
    "    gp_params = (1.0, length_scale)\n",
    "    jitter = 1e-10\n",
    "    X =jnp.linspace(0, 1, N)[:,None]\n",
    "    K = RBF(X, X, gp_params)\n",
    "    L =jnp.linalg.cholesky(K + jitter*np.eye(N))\n",
    "    gp_sample =jnp.dot(L, jax.random.normal(key, (N,)))\n",
    "\n",
    "    # Create a callable interpolation function  \n",
    "    u_fn = lambda x, t:jnp.interp(t, X.flatten(), gp_sample)\n",
    "\n",
    "    # Input sensor locations and measurements\n",
    "    x =jnp.linspace(0, 1, m)\n",
    "    u = jax.vmap(u_fn, in_axes=(None,0))(0.0, x)\n",
    "\n",
    "    # Output sensor locations and measurements\n",
    "    y_train = jax.random.uniform(key, (P,)).sort() \n",
    "    v_train = odeint(u_fn, 0.0,jnp.hstack((0.0, y_train)))[1:] # JAX has a bug and always returns s(0), so add a dummy entry to y and return s[1:]\n",
    "\n",
    "    # Tile inputs\n",
    "    u_train =jnp.tile(u, (P,1))\n",
    "\n",
    "    return u_train, y_train, v_train\n",
    "\n",
    "# Geneate test data corresponding to one input sample\n",
    "def generate_one_test_data(key, m=100, P=100,\n",
    "                           length_scale=0.2):\n",
    "    # Sample GP prior at a fine grid\n",
    "    N = 512\n",
    "    gp_params = (1.0, length_scale)\n",
    "    jitter = 1e-10\n",
    "    X =jnp.linspace(0, 1, N)[:,None]\n",
    "    K = RBF(X, X, gp_params)\n",
    "    L =jnp.linalg.cholesky(K + jitter*np.eye(N))\n",
    "    gp_sample =jnp.dot(L, jax.random.normal(key, (N,)))\n",
    "\n",
    "    # Create a callable interpolation function  \n",
    "    u_fn = lambda x, t:jnp.interp(t, X.flatten(), gp_sample)\n",
    "\n",
    "    # Input sensor locations and measurements\n",
    "    x =jnp.linspace(0, 1, m)\n",
    "    u = jax.vmap(u_fn, in_axes=(None,0))(0.0, x)\n",
    "\n",
    "    # Output sensor locations and measurements\n",
    "    y =jnp.linspace(0, 1, P)\n",
    "    v = odeint(u_fn, 0.0, y)\n",
    "\n",
    "    # Tile inputs\n",
    "    u =jnp.tile(u, (P,1))\n",
    "\n",
    "    return u, y, v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "N_train = 10000 #Number of functions\n",
    "m = 100 # number of input sensors\n",
    "P_train = 1   # number of output sensors\n",
    "key_train = jax.random.PRNGKey(0)  # use different key for generating training data and test data \n",
    "config.update(\"jax_enable_x64\", True) # Enable double precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_train = jax.random.split(key_train, N_train) # Obtain 10000 random numbers\n",
    "gen_fn = jax.jit(lambda key: generate_one_training_data(key, m, P_train)) #lets call our function\n",
    "u_train, y_train, v_train = map(np.array,jax.vmap(gen_fn)(keys_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n",
      "(10000, 1, 1)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "u_train = u_train.reshape(N_train * P_train,-1)\n",
    "y_train = y_train.reshape(N_train,-1,P_train)\n",
    "v_train = v_train.reshape(N_train * P_train,-1)\n",
    "print(u_train.shape)\n",
    "print(y_train.shape)\n",
    "print(v_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Data\n",
    "N_test = 1 # number of input samples \n",
    "P_test = m   # number of sensors \n",
    "key_test = jax.random.PRNGKey(12345) # A different key \n",
    "\n",
    "keys_test = jax.random.split(key_test, N_test)\n",
    "gen_fn = jax.jit(lambda key: generate_one_test_data(key, m, P_test))\n",
    "u_test, y_test, v_test = map(np.array,jax.vmap(gen_fn)(keys_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(1, 1, 100)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "u_test = u_test.reshape(N_test * P_test,-1)\n",
    "y_test = y_test.reshape(N_test,-1,P_test)\n",
    "v_test = v_test.reshape(N_test * P_test,-1)\n",
    "print(u_test.shape)\n",
    "print(y_test.shape)\n",
    "print(v_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.has_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(dt.Dataset):\n",
    "    \n",
    "    def __init__(self,inputs:torch.Tensor,location:torch.Tensor,outputs:torch.Tensor):\n",
    "        self.input_signals = inputs\n",
    "        self.collocation_points = location\n",
    "        self.output_signals = outputs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_signals)\n",
    "    \n",
    "    def __getitem__(self, index) -> tuple:\n",
    "        v = self.input_signals[index,:]\n",
    "        y = self.collocation_points[index,:,:]\n",
    "        u = self.output_signals[index,:]\n",
    "        return ((v,y),u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = DataGenerator(u_train,y_train,v_train)\n",
    "training_loader = dt.DataLoader(training_set,batch_size = 32,\n",
    "                                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100])\n",
      "torch.Size([32, 1, 1])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for ((b_in,y_loc),d_out) in training_loader:\n",
    "    print(b_in.shape)\n",
    "    print(y_loc.shape)\n",
    "    print(d_out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    \"\"\"Base class for neural network modules\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.regulariser = None\n",
    "    @property\n",
    "    def num_trainable_parameters(self):\n",
    "        \"\"\"Evaluate number of trainable parameters for NN\"\"\"\n",
    "        return sum(v.numel() for v in self.parameters() if v.requires_grad)\n",
    "#%%\n",
    "class MLP(NN):\n",
    "    \"\"\"Mulilayer perceptron network fully connected\"\"\"\n",
    "    def __init__(self,layer_sizes,\n",
    "                 activation = nn.ReLU(),\n",
    "                 **init_kwargs):\n",
    "        super().__init__()\n",
    "        self.activation = activation       \n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(l_in,l_out,dtype = torch.float32) \n",
    "                for (l_in,l_out) in zip(layer_sizes,layer_sizes[1:])])\n",
    "\n",
    "        self.apply(self._init_weights,**init_kwargs)\n",
    "        \n",
    "    def _init_weights(self,module:nn.Linear,initialiser = nn.init.xavier_normal_,\n",
    "                      zero_initialiser = nn.init.zeros_):\n",
    "        if isinstance(module,nn.Linear):\n",
    "            initialiser(module.weight)\n",
    "            zero_initialiser(module.bias)   \n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        \n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepONet(NN):\n",
    "    \"\"\"Deep operator network for dataset in the format of Cartesian product.\n",
    "\n",
    "    Args:\n",
    "        layer_sizes_branch: A list of integers as the width of a fully connected network,\n",
    "            or `(dim, f)` where `dim` is the input dimension and `f` is a network\n",
    "            function. The width of the last layer in the branch and trunk net should be\n",
    "            equal.\n",
    "        layer_sizes_trunk (list): A list of integers as the width of a fully connected\n",
    "            network.\n",
    "        activation: If `activation` is a ``string``, then the same activation is used in\n",
    "            both trunk and branch nets. If `activation` is a ``dict``, then the trunk\n",
    "            net uses the activation `activation[\"trunk\"]`, and the branch net uses\n",
    "            `activation[\"branch\"]`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes_branch:list,\n",
    "        layer_sizes_trunk:list,\n",
    "        *args,**kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        #activation_branch = activation_trunk = activation\n",
    "\n",
    "        self.branch = MLP(layer_sizes_branch, *args,**kwargs)\n",
    "        self.trunk = MLP(layer_sizes_trunk, *args,**kwargs)\n",
    "        \n",
    "        self.b = torch.tensor(0.0,requires_grad = True)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        v_func:torch.Tensor = inputs[0] # Input signal (batch_size,resolution)\n",
    "        y_loc:torch.Tensor = inputs[1].swapaxes(2,1) # Collocation points (batch_size,input_dim,num_points)->(b,p,n)\n",
    "        print(v_func.shape)\n",
    "        # Branch net to encode the input function\n",
    "        v_func = self.branch(v_func)\n",
    "        \n",
    "        # Trunk net to encode the domain of the output function\n",
    "        y_loc = self.trunk(y_loc).swapaxes(2,1) #Output dim (batch_size,output_layer_dim,num_points)\n",
    "        \n",
    "        # Dot product\n",
    "        if v_func.shape[-1] != y_loc.shape[1]:\n",
    "            raise AssertionError(\n",
    "                \"Output sizes of branch net and trunk net do not match.\")\n",
    "        x = torch.einsum(\"bl,blp->bp\", v_func, y_loc)\n",
    "        \n",
    "        # Add bias\n",
    "        x += self.b\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a network\n",
    "m = 100\n",
    "dim_x = 1\n",
    "net = DeepONet(\n",
    "    [m, 40, 40],\n",
    "    [dim_x, 40, 40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.branch.layers[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "niter = 10000\n",
    "opt = optim.Adam(net.parameters(),lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2000 % 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "torch.Size([32, 100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m i,data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(training_loader):\n\u001b[0;32m     10\u001b[0m     inputs,labels \u001b[39m=\u001b[39m data\n\u001b[1;32m---> 11\u001b[0m     output \u001b[39m=\u001b[39m net(inputs)\n\u001b[0;32m     13\u001b[0m     opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     15\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(output,labels)\n",
      "File \u001b[1;32mc:\\Users\\1833097\\Miniconda3\\envs\\jax_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[37], line 36\u001b[0m, in \u001b[0;36mDeepONet.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mprint\u001b[39m(v_func\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     35\u001b[0m \u001b[39m# Branch net to encode the input function\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m v_func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbranch(v_func)\n\u001b[0;32m     38\u001b[0m \u001b[39m# Trunk net to encode the domain of the output function\u001b[39;00m\n\u001b[0;32m     39\u001b[0m y_loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrunk(y_loc)\u001b[39m.\u001b[39mswapaxes(\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m) \u001b[39m#Output dim (batch_size,output_layer_dim,num_points)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\1833097\\Miniconda3\\envs\\jax_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[29], line 33\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     31\u001b[0m x \u001b[39m=\u001b[39m inputs\n\u001b[0;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m---> 33\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(layer(x))\n\u001b[0;32m     34\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m](x)\n\u001b[0;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n",
      "File \u001b[1;32mc:\\Users\\1833097\\Miniconda3\\envs\\jax_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\1833097\\Miniconda3\\envs\\jax_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    print(f'EPOCH {epoch}')\n",
    "    \n",
    "    net.train(True)\n",
    "    \n",
    "    running_loss = 0.\n",
    "    last_loss = 0. \n",
    "    \n",
    "    for i,data in enumerate(training_loader):\n",
    "        inputs,labels = data\n",
    "        output = net(inputs)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        loss = loss_fn(output,labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i%50 ==49:\n",
    "            last_loss = running_loss/32\n",
    "            print(f'Batch :{i} \\t Loss\\t{last_loss:.5f}')\n",
    "            running_loss = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_func,y_loc  = inputs\n",
    "v_func.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m net\u001b[39m.\u001b[39;49mbranch(v_func)\n",
      "File \u001b[1;32mc:\\Users\\1833097\\Miniconda3\\envs\\jax_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[29], line 33\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     31\u001b[0m x \u001b[39m=\u001b[39m inputs\n\u001b[0;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m---> 33\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(layer(x))\n\u001b[0;32m     34\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m](x)\n\u001b[0;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n",
      "File \u001b[1;32mc:\\Users\\1833097\\Miniconda3\\envs\\jax_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\1833097\\Miniconda3\\envs\\jax_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "net.branch(v_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(v_func.dtype)\n",
    "for layer in net.branch.layers:\n",
    "    print(layer.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a Model\n",
    "model = dde.Model(data, net)\n",
    "\n",
    "# Compile and Train\n",
    "model.compile(\"adam\", lr=0.001, metrics=[\"mean l2 relative error\"])\n",
    "losshistory, train_state = model.train(iterations=10000)\n",
    "\n",
    "# Plot the loss trajectory\n",
    "dde.utils.plot_loss_history(losshistory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
